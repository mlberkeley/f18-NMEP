{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STYLE TRANSFER\n",
    "\n",
    "##### By: (The one and only) James Bartlett, Edited by Ashley Chien\n",
    "\n",
    "In this project we are going to be using Convolutional Neural Networks to implement Neural Style Transfer, a technique for creating a new image with the content of one input image and the style of another input. The idea behind style transfer is as follows: take three input images, one our style image, one our content image, and one output image which starts as random noise and we iteratively update it until it looks like the content of the content image in the style of the style image. To do this we run all three images through a pretrained VGG16 model trained to classify images. Then for a selected convolutional or pooling layer of the VGG16 model we compare the activations (values of the neurons) at that layer for the three different images. Specifically, we have what is called a feature reconstruction loss that compares the activations of the current output image and the content image, and what is called a style loss that compares the activations of the current output image and the desired style image. Then we use the gradient of these loss functions to update our current output image. Hopefully, that gives you an overview of what we will be doing, and you should gain a more in depth understanding as we go along.  \n",
    "\n",
    "### The paper we will be implementing is found here: https://arxiv.org/pdf/1508.06576.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Build Model and Define Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we want to initialize a VGG16 model we can use for style transfer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "content_input = Input(batch_shape=(1, 224, 224, 3))\n",
    "style_input = Input(batch_shape=(1, 224, 224, 3))\n",
    "output_tensor = tf.get_variable(\"output_tensor\", [1, 224, 224, 3])\n",
    "output_input = Input(tensor=output_tensor)\n",
    "## TODO: use a concatenate layer to concatenate the three inputs on the first axis.\n",
    "input_tensor = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error for the cell below about SSL PROTOCOL VERSIONS or something similar you can try downloading this file\n",
    "https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "and putting it in the folder `~/.keras/models` on your computer. Then the cell below should work after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a pretrained VGG 16 model, which is really easy to do in Keras\n",
    "# include_top=False ensures we don't use the fully connected layers.\n",
    "vgg_model = vgg16.VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "# We can now look at the structure of this model\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([layer.name for layer in vgg_model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now select one of the above listed layers to be the layer to use for content information\n",
    "# and select some number of layers (maybe 2 or 3 layers) from the above layers to be the\n",
    "# style information. If you choose layers closer to the input, this will use \n",
    "# more simplistic features, and choosing layers closer to the end will use more complicated\n",
    "# abstracted features.\n",
    "content_layer = ???\n",
    "style_layers = [\n",
    "    ???,\n",
    "    ???\n",
    "]\n",
    "# You can also play with the content and style loss weights if you want to. This will affect \n",
    "# how stylized vs similar to the content image the output will look.\n",
    "content_loss_weight = 5.0\n",
    "style_loss_weight = 500.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = dict([(layer.name, layer.output) for layer in vgg_model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "We want to define our style transfer losses now. First, we are going to define a feature reconstruction loss based on our content features and our output features. Using tensorflow functions, implement the following loss function: $$\\frac{1}{2} \\sum_{i,j, k} (F_{ijk} - P_{ijk})^2$$ where $F$ is the 3D tensor of content features and $P$ is the 3D tensor of our output image features. HINT: `tf.reduce_sum` and `tf.square` will be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reconstruction_loss(content_img_features, output_img_features):\n",
    "    \"\"\"Takes a tensor representing a layer of VGG features from the content image\n",
    "    and a tensor representing a layer of VGG features from the current output image and returns a loss value.\n",
    "    \"\"\"\n",
    "    # TODO: YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to define our style loss function. First, we have to take our features and represent them as a Gram Matrix, for more information on Gram Matrices and this loss function you can read the paper if you like. Then we wish to implement the loss function:\n",
    "$$ \\frac{1}{4H^2W^2C^2} \\sum_{ij} (G_{ij} - A_{ij})^2 $$ where $G$ is the Gram matrix of the output image features and $A$ is the Gram Matrix of the style image features. Note that we have written a Gram matrix function for you so you only need to call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    # make channels first dimension\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    # flatten everything but channels so x is now (C, H*W)\n",
    "    x = tf.reshape(x, tf.stack([-1, tf.reduce_prod(tf.shape(x)[1:])]))\n",
    "    return tf.matmul(x, tf.transpose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style_img_features, output_img_features, img_shape):\n",
    "    \"\"\"Takes a tensor representing a layer of VGG features from the style image and a tensor\n",
    "    representing a layer of VGG features from the current output image and returns \n",
    "    the style loss for these features.\n",
    "    \"\"\"\n",
    "    # TODO: YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_features = layers_dict[content_layer]\n",
    "content_img_features = content_features[0, :, :, :]\n",
    "output_content_features = content_features[2, :, :, :]\n",
    "content_loss = feature_reconstruction_loss(content_img_features, output_content_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_style_loss = tf.zeros(1)\n",
    "weight = 1.0 / len(style_layers)\n",
    "for style_layer in style_layers:\n",
    "    style_features = layers_dict[style_layer]\n",
    "    style_img_features = style_features[1, :, :, :]\n",
    "    output_img_features = style_features[2, :, :, :]\n",
    "    total_style_loss += weight * style_loss(style_img_features, output_img_features, (224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to combine our two loss functions using the weightings we defined earlier. \n",
    "\n",
    "HINT: Don't overthink this; it should be a very simple operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = tf.train.AdamOptimizer(learning_rate=10).minimize(total_loss, var_list=[output_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feeding in Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to load and preprocess our images. keras provides a `load_img` function that conveniently loads our image and then cuts it down to our target size. Keras also provides a `vgg16.preprocess_input` that preprocesses images to be in the format vgg16 expects. Use these two functions to write the load_image function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "def load_image(img_path):\n",
    "    img = #YOUR CODE HERE call load img and set the target size to be (224,224,3)\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = #YOUR CODE HERE\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x = x.reshape((224, 224, 3))\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img_path = 'images/campanile.jpg'\n",
    "style_img_path = 'images/monet_style.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = load_image(content_img_path)\n",
    "style_img = load_image(style_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(deprocess_image(content_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(deprocess_image(style_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Stylize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_var = tf.assign(output_tensor, content_img)\n",
    "sess = K.get_session()\n",
    "var = sess.run(assign_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell below will update the image 10 times. Since the initialization code is in the cell above, if you run the cell below and your output isn't great, you can run it for another 10 iterations simply by rerunning the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 10\n",
    "for i in range(n_iterations):\n",
    "    print(\"Running iteration: {}\".format(i))\n",
    "    _, output_val, loss = sess.run([optimize, output_tensor, total_loss], feed_dict={content_input: content_img, style_input: style_img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_img = deprocess_image(output_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Style Transfer Writeup\n",
    "\n",
    "Now you need to write-up your project. First, write a short paragraph about your understanding of how style transfer works. Feel free to refer to the paper if it helps but your paragraph needs to be in your own words. \n",
    "\n",
    "Then attach 3 sets of images to your writeup. For each set show the original content image, the original style image, and the style transfer result. One set should be the images we provided here, include the content and style layers you used as well as the content and style weights you used. Another set should be the images we provided here but with different content layers, style layers and different content and style weights, include your choices for the layers and weights in your writeup. Finally, include a set of images that is based on a new content image and a new style image that you choose yourself. There will be an award for the group with the coolest style transfer result. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
