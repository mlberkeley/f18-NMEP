{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose:\n",
    "Identify numbers (MNIST data) using SVCs and kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set-up \n",
    "\n",
    "Import data and use train_test_split to set up training and testing datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import train_test_split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "# print shapes of X and y\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# set test size to 25%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PCA from sklearn\n",
    "We will use Principal Component Analysis (PCA) to manipulate the data to make it more usable for SVC. The main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52500, 43)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Keep 80% of feature variation. Get rid of the rest.\n",
    "pca = PCA(n_components=0.8, whiten=True)\n",
    "\n",
    "# Use pca to get new X_train, X_test\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "#print the shape of X_train_pca \n",
    "print(X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What change do you notice between our old training data and our new one?\n",
    "\n",
    "Answer: We reduced the number of features or input data but still retained 80% of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC and Kernels\n",
    "\n",
    "Now we will experiment with support vector classifiers and kernels. We will need LinearSVC, SVC, and accuracy_score.\n",
    "\n",
    "SVMs are really interesting because they have something called the dual formulation, in which the computation is expressed as training point inner products. This means that data can be lifted into higher dimensions easily with this \"kernel trick\". Data that is not linearly separable in a lower dimension can be linearly separable in a higher dimension - which is why we conduct the transform. Let us experiment.\n",
    "A transformation that lifts the data into a higher-dimensional space is called a kernel. A poly- nomial kernel expands the feature space by computing all the polynomial cross terms to a specific degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.892476190476\n",
      "test acc:  0.894057142857\n",
      "train acc:  0.9896\n",
      "test acc:  0.980914285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fit the LinearSVC on X_train_pca and y_train and then print train accuracy and test accuracy\n",
    "\n",
    "lsvc = LinearSVC(dual=False, tol=0.01) \n",
    "lsvc.fit(X_train_pca, y_train)\n",
    "print('train acc: ', accuracy_score(lsvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(lsvc.predict(X_test_pca), y_test))\n",
    "        \n",
    "# use SVC with a polynomial kernel. Fit this model on X_train_pca and y_train and print accuracy metrics as before\n",
    "\n",
    "psvc = SVC(kernel='poly', degree=2, tol=0.01, cache_size=4000) \n",
    "psvc.fit(X_train_pca, y_train, sample_weight = None)\n",
    "print('train acc: ', accuracy_score(psvc.predict(X_train_pca), y_train)) \n",
    "print('test acc: ', accuracy_score(psvc.predict(X_test_pca), y_test))\n",
    "\n",
    "# play around with the degree of the polynomial to see if you can improve your accuracy \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel uses the gaussian function to create an infinite dimensional space - a gaussian peak at each datapoint. Use this http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html to figure out what gamma and C paramaters are the most optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SVC with rbf kernel. Fit this model on X_train_pca and y_train and print accuracy metrics.\n",
    "\n",
    "rsvc = SVC(kernel='rbf', tol=0.01, cache_size=400, C = 200 , gamma = 10)\n",
    "rsvc.fit(X_train_pca, y_train)\n",
    "print('train acc: ', accuracy_score(rsvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(rsvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression \n",
    "\n",
    "Let's see what happens when we do ridge regression with the kernel trick. We will use KernelRidge from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# use KernelRidge to see if accuracy scores are improved. \n",
    "krr = KernelRidge(alpha=1.0)\n",
    "krr.fit(X_train_pca, y_train) \n",
    "print('train acc: ', accuracy_score(krr.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(krr.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1) What is a kernel and why is it important?\n",
    "\n",
    "2) Can we kernelize all types of data? Why or why not?\n",
    "\n",
    "3) What are some pros/cons of kernels? (look into runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
