{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioned Language Models\n",
    "In this homework, you will be implementing a language model that generates texts based off of ML Medium articles. This language model will be conditioned on the title of article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "A language model predicts a word at time t *w<sub>t</sub>* given the previously generated words *w<sub>t-K</sub>*, *w<sub>t-K+1</sub>*, ..., *w<sub>t-1</sub>*, where *K* is a given window size. We will be implementing this by using an RNN, feeding in the previous *K* words sequentially as input and doing a classification among all possible words in the vocabulary.  \n",
    "\n",
    "We will also be conditioning this RNN with a vector representation of the title of the given article. While there are many ways we can do this, for this homework we will do so by running a bidirectional RNN over the title and combining the outputs together to form a single vector that we will initialize the language model RNN with.  \n",
    "\n",
    "**Note:** This model is not expected to get stellar results, and is not based off of any state of the art work. The purpose of building this architecture is to familiarize yourself with various ways to implement and combine different layers together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with some imports that we'll be using for text preprocessing, training, and inference. Additionally, we'll be defining some constants for different parts of the model. Feel free to modify the constants as needed or desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 256\n",
    "BEAM_SEARCH_SIZE = 10\n",
    "CONTEXT_LSTM_UNITS = 128\n",
    "DECODE_LENGTH = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "LOG_DIR = 'logs'\n",
    "LOG_RATE = 10\n",
    "NUM_EPOCHS = 3\n",
    "TITLE_LSTM_UNITS = 128\n",
    "WINDOW_LENGTH = 15\n",
    "WORDVEC_DIM = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll be importing our data and running preprocessing to extract tokenized titles and the raw text of the article. We will also be using gensim's FastText implementation to get word embeddings for each of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = []\n",
    "first_line = True\n",
    "with open('articles.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        if first_line:\n",
    "            first_line = False\n",
    "        else:\n",
    "            raw_text.append(row)\n",
    "\n",
    "# List of article titles, where each element is a list of tokenized words in each title\n",
    "titles = [nltk.word_tokenize(raw_text[i][4]) for i in range(len(raw_text))]\n",
    "\n",
    "# Intermediary list for tokenized text extraction\n",
    "sent_texts = [[s.replace('\\n', ' ') for s in nltk.sent_tokenize(raw_text[i][5])] for i in range(len(raw_text))]\n",
    "\n",
    "# List of tokenized article content, where each element is a list of sentences containing tokenized words\n",
    "tokenized = [[nltk.word_tokenize(sent.lower()) for sent in text] for text in sent_texts]\n",
    "\n",
    "# Object holding FastText word embeddings for our corpus of words\n",
    "ft = FastText(list(itertools.chain(*tokenized)) + [[w.lower() for w in t] for t in titles],\n",
    "              size=WORDVEC_DIM, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data now read, we can define a couple more values that will be needed in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_length = max([len(t) for t in titles])\n",
    "vocab_size = len(ft.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data processed, we can generate our data samples to be fed in during training. Each sample will be a tuple of `(article title, K-window, target word)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "flat_docs = [list(itertools.chain(*text)) for text in tokenized]\n",
    "for i in range(len(flat_docs)):\n",
    "    for j in range(len(flat_docs[i])):\n",
    "        data.append((titles[i], flat_docs[i][max(j-WINDOW_LENGTH,0):j], flat_docs[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Batch Generation\n",
    "Despite all our preprocessing and generating all samples, there is still more that needs to be done before we can directly feed our data into our model for training. In particular, there are a few things that we need to do:\n",
    "1. Shuffle the data at each epoch\n",
    "2. Convert the words in the data into an array of word vectors\n",
    "3. Convert the target word into an index\n",
    "4. Pad the title and window sequences with null values so all arrays are of equal size \n",
    "5. Extract sequence length for RNN masking\n",
    "6. Separate the data into batches\n",
    "\n",
    "For this part, you need to implement a generator over a single epoch of data.  \n",
    "The generator should yield a 5-tuple `(title, window, title length, window length, target index)`, each of which should be a numpy array. The shape of `title` should be `[BATCH_SIZE, title_length, WORDVEC_DIM]`, `window` should be `[BATCH_SIZE, WINDOW_LENGTH, WORDVEC_DIM]`, and `title length`, `window length`, and `target index` should all be `[BATCH_SIZE]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data, wordvecs):\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to start building the model! We will begin by defining the placeholders corresponding to the data we will be inputting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_input = tf.placeholder(tf.float32, [None, title_length, WORDVEC_DIM])\n",
    "title_seqlen_input = tf.placeholder(tf.int32, [None])\n",
    "context_input = tf.placeholder(tf.float32, [None , WINDOW_LENGTH, WORDVEC_DIM])\n",
    "context_seqlen_input = tf.placeholder(tf.int32, [None])\n",
    "target_idx_input = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Bidirectional RNN for Conditioning on Titles\n",
    "We will now be constructing the bidirectional RNNs for the title for use in conditioning the language model. There are 4 steps in doing so:\n",
    "1. Create the forward and backward LSTM cells using `tf.nn.rnn_cell.LSTMCell`\n",
    "2. Pass the title into `tf.nn.bidirectional_dynamic_rnn` using the LSTM cells that you have created\n",
    "3. Take the mean over timesteps of all outputs of the Bidirectional RNN\n",
    "4. Transform the vector with a linear layer to fit the dimensions of the language model RNN\n",
    "\n",
    "Assign the variable `title_hidden` to the final of these layers. The constants `TITLE_LSTM_UNITS` and `CONTEXT_LSTM_UNITS` will be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\"\n",
    "title_hidden = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: RNN Language Model\n",
    "With the bidirectional RNN constructed, we can now proceed with the language model RNN. For this part, not only will you need to construct the RNN, but you'll also need to create the loss and optimizer TensorFlow operations. You can do so following the steps below:\n",
    "1. As with the Bidirectional RNN, create the LSTM cell (only a single one is needed this time)\n",
    "2. Initialize the cell state to be `title_hidden` (see documentation relating to `tf.nn.rnn_cell.LSTMStateTuple`)\n",
    "3. Pass the `context_input` through the RNN and retrieve the last output\n",
    "4. Feed said output through a linear layer to compute logits for every word in the vocab\n",
    "5. Use a cross entropy loss between the predicted words and target labels\n",
    "6. Create an optimizer that minimizes over the loss\n",
    "\n",
    "You can use `vocab_size` to determine the size of the embeddings to predict over, and `LEARNING_RATE` for the opitimizer learning rate.  \n",
    "Assign your predicted logits to `pred`, loss to `loss`, and optimization operation to `train_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\"\n",
    "pred = None\n",
    "loss = None\n",
    "train_step = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to look at TensorBoard logging graphs to keep track of training, so I've added code for that below. Feel free to add other summarization or related ops as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "run_num = 0\n",
    "while os.path.isdir(os.path.join(LOG_DIR, 'run_%d' % run_num)):\n",
    "    run_num += 1\n",
    "\n",
    "tf.summary.scalar('model_loss', loss)\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'run_%d' % run_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to training, we begin a new session with `tf.Session()` and initialize all variables with `tf.global_variables_initializer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data ready to be batched and model constructed, we can finally begin training. Run the code block below, and feel free to start a TensorBoard server to keep track of progress.  \n",
    "**Note:** Training the model may take a considerable amount of time (only my laptop it takes nearly half an hour per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Beginning training...')\n",
    "step = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    batches = generate_batch(data, ft)\n",
    "    while(True):\n",
    "        try:\n",
    "            batch_title, batch_context, batch_titlelen, batch_contextlen, batch_target = next(batches)\n",
    "            feed_dict = {title_input: batch_title, title_seqlen_input: batch_titlelen,\n",
    "                         context_input: batch_context, context_seqlen_input: batch_contextlen,\n",
    "                         target_idx_input: batch_target}\n",
    "            if step % LOG_RATE == 0:\n",
    "                _, step_loss, summary = sess.run([train_step, loss, merged], feed_dict)\n",
    "                writer.add_summary(summary, global_step=step)\n",
    "            else:\n",
    "                _, step_loss = sess.run([train_step, loss], feed_dict)\n",
    "            step += 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "    print('Epoch %02d complete, current loss is %.5f' % (epoch, step_loss))\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is done, we'd want to try to generate some text of our own to see what kind of outputs we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_k = tf.placeholder(tf.int32, [])\n",
    "top_vals, top_idx = tf.nn.top_k(pred, decode_k)\n",
    "\n",
    "# Since there is no easy way of getting a word from an index with the word vector object, we'll manually create\n",
    "# our own dictionary for the mapping\n",
    "idx2word = {ft.wv.vocab.get(k).index: k for k in ft.wv.vocab.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Beam Search Decoder\n",
    "While a simple greedy decoding (only take the most probable word when decoding) is sufficient to get a generated sequence, using beam search can more easily decode longer word dependencies that a simple greedy approach may not. The beam search decoding works as follows:\n",
    "1. Take the top N most probable existing sequences (or if the initial sequence, the single empty sequence)\n",
    "2. Compute the logits for all possible following words for each sequences\n",
    "3. Add the logits to a running sum of all possible logits for the sequence\n",
    "4. Rank all the new sequences by the updated logit sums, and keep the top N\n",
    "5. Repeat until the sequence is fully generated\n",
    "6. Keep the sequence with the highest logit sum (directly correlated with the generation probability)\n",
    "\n",
    "Implement the `beam_search_decode` function to generate text of `DECODE_LENGTH` words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(title, wordvecs):\n",
    "    sequences = [([], 0)]\n",
    "    if isinstance(title, str):\n",
    "        title = nltk.word_tokenize(title.lower())\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, even after training our model doesn't perform all too well. This isn't surprising, since you're likely running off of a laptop, using a single layer RNN on a very limited dataset. However, I hope that through this you have gained some experience working with RNNs and textual data in an actual coding setting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
